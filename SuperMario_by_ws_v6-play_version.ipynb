{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  강화학습을 이용한 인공지능 슈퍼마리오 만들기 메뉴얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 메뉴얼은 슈퍼마리오 환경 설치부터 강화학습 알고리즘(DQN)을 이용해 똑똑한 '슈퍼'마리오를 만들기 위한 메뉴얼 입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make my OWN! A.I. SuperMario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym \n",
    "from wrapper import action_space\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Input, Lambda, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential\n",
    "#from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sum_tree import SumTree\n",
    "import random\n",
    "\n",
    "\n",
    "load_model = True\n",
    "\n",
    "epsilon = 1.\n",
    "epsilon_start, epsilon_end = 1.0, 0.1\n",
    "exploration_steps = 400000.\n",
    "\n",
    "epsilon_decay_step = (epsilon_start - epsilon_end) \\\n",
    "                                  / exploration_steps\n",
    "batch_size = 32\n",
    "\n",
    "discount_factor = 0.99\n",
    "\n",
    "train_start= 100\n",
    "update_target_rate = 10000\n",
    "memory=deque(maxlen=1000000)\n",
    "just_start_train=True\n",
    "state_size = (84,84,4)\n",
    "action_size =6\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(8,8),strides =(4,4), activation='relu', input_shape=state_size))\n",
    "    model.add(Conv2D(64,(4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3), strides =(1,1), activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(action_size))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "target_model=build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change action(number) to the action( action what I think I need )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "action_size=6\n",
    "epsilon = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an greedy action\n",
    "epsilon=0.9\n",
    "action_size=6\n",
    "def get_action(history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        else:\n",
    "            q_value = model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(history, action, reward, next_history, dead):\n",
    "    memory.append((history,action,reward,next_history,dead))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(model.trainable_weights, [], loss)\n",
    "        train = K.function([model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model():\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "(Batch from the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.9\n",
    "\n",
    "def train_model():\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon -= epsilon_decay_step\n",
    "    \n",
    "    mini_batch= random.sample(memory, batch_size)\n",
    "    #mini_batch = memory.sample(batch_size)\n",
    "    \n",
    "  \n",
    "    history = np.zeros((batch_size, state_size[0],\n",
    "                            state_size[1], state_size[2]))\n",
    "    next_history = np.zeros((batch_size, state_size[0],\n",
    "                             state_size[1], state_size[2]))\n",
    "    target = np.zeros((batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "    target_value = target_model.predict(next_history)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = reward[i]\n",
    "        else:\n",
    "            target[i] = reward[i] + discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "    loss = optimizer([history, action, target])\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LET's make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "wra_act=action_space\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)\n",
    "\n",
    "#reduce inputsize to 84,84,1\n",
    "env=wra_act.ProcessFrame84(env)\n",
    "\n",
    "\n",
    "obs=env.reset()\n",
    "model = build_model()\n",
    "\n",
    "load_model(\"./save_ws/supermario_again_ws_v6.h5\")\n",
    "\n",
    "for e in range(10000):\n",
    "        done = False\n",
    "        \n",
    "        step, score, start_life = 0, 0, 5\n",
    "        \n",
    "        \n",
    "        \n",
    "        reshape_obs=np.reshape([obs],(1,84,84,1))\n",
    "        history=np.stack((obs,obs,obs,obs), axis = 2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        history=np.append(reshape_obs, history[:,:,:,:3], axis=3)\n",
    "        #check\n",
    "       # history=np.reshape([history],(84,84,4))\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "        while not done:\n",
    "           \n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 0: stop, 3: left, 4: left jump, 7:right, 8:right jump 11: jump\n",
    "            action=get_action(history)\n",
    "            if action==0:\n",
    "                actions=[0,0,0,1,0,0]\n",
    "            elif action==2:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "    \n",
    "            elif action==1:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==3:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==4:\n",
    "                actions=[0,0,0,1,0,0]\n",
    "    \n",
    "            elif action==5:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==6:\n",
    "                actions=[0,0,0,0,1,0]\n",
    "    \n",
    "            env.step(actions)\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, clear= env.step(actions)\n",
    "         #   if clear:\n",
    "          #      reward += 30\n",
    "           #     done = True\n",
    "\n",
    "          #  if done and not clear:\n",
    "            #    reward = -30\n",
    "              \n",
    "\n",
    "           # reward /= 30\n",
    "          #  reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "         #   observe = wra_act.mario_action(observe)\n",
    "         #   observe=wra_act.ProcessFrame84(observe)\n",
    "            \n",
    "            next_state = observe\n",
    "            \n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            \n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            #check\n",
    "           # next_history = np.reshape([history], (84, 84, 4))\n",
    "\n",
    "         #   agent.avg_q_max += np.amax(\n",
    "       #         agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "            \n",
    "            append_sample(history, action, reward, next_history, done)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            history = next_history\n",
    "            \n",
    " \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
