{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  강화학습을 이용한 인공지능 슈퍼마리오 만들기 메뉴얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 메뉴얼은 슈퍼마리오 환경 설치부터 강화학습 알고리즘(DQN)을 이용해 똑똑한 '슈퍼'마리오를 만들기 위한 메뉴얼 입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  슈퍼마리오 환경 불러오기\n",
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "\n",
    "슈퍼마리오 환경을 임포트 합니다.\n",
    "\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')\n",
    "\n",
    "gym의 함수인 make 함수를 사용하여 위에서 불러온 환경을 현재 환경으로 지정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ppaquette/meta-SuperMarioBros-v0\n",
    "    ppaquette/meta-SuperMarioBros-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-1-1-v0\n",
    "    ppaquette/SuperMarioBros-1-2-v0\n",
    "    ppaquette/SuperMarioBros-1-3-v0\n",
    "    ppaquette/SuperMarioBros-1-4-v0\n",
    "    ppaquette/SuperMarioBros-2-1-v0\n",
    "    ppaquette/SuperMarioBros-2-2-v0\n",
    "    ppaquette/SuperMarioBros-2-3-v0\n",
    "    ppaquette/SuperMarioBros-2-4-v0\n",
    "    ppaquette/SuperMarioBros-3-1-v0\n",
    "    ppaquette/SuperMarioBros-3-2-v0\n",
    "    ppaquette/SuperMarioBros-3-3-v0\n",
    "    ppaquette/SuperMarioBros-3-4-v0\n",
    "    ppaquette/SuperMarioBros-4-1-v0\n",
    "    ppaquette/SuperMarioBros-4-2-v0\n",
    "    ppaquette/SuperMarioBros-4-3-v0\n",
    "    ppaquette/SuperMarioBros-4-4-v0\n",
    "    ppaquette/SuperMarioBros-5-1-v0\n",
    "    ppaquette/SuperMarioBros-5-2-v0\n",
    "    ppaquette/SuperMarioBros-5-3-v0\n",
    "    ppaquette/SuperMarioBros-5-4-v0\n",
    "    ppaquette/SuperMarioBros-6-1-v0\n",
    "    ppaquette/SuperMarioBros-6-2-v0\n",
    "    ppaquette/SuperMarioBros-6-3-v0\n",
    "    ppaquette/SuperMarioBros-6-4-v0\n",
    "    ppaquette/SuperMarioBros-7-1-v0\n",
    "    ppaquette/SuperMarioBros-7-2-v0\n",
    "    ppaquette/SuperMarioBros-7-3-v0\n",
    "    ppaquette/SuperMarioBros-7-4-v0\n",
    "    ppaquette/SuperMarioBros-8-1-v0\n",
    "    ppaquette/SuperMarioBros-8-2-v0\n",
    "    ppaquette/SuperMarioBros-8-3-v0\n",
    "    ppaquette/SuperMarioBros-8-4-v0\n",
    "    ppaquette/SuperMarioBros-1-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-1-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-1-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-1-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-2-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-2-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-2-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-2-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-3-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-3-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-3-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-3-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-4-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-4-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-4-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-4-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-5-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-5-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-5-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-5-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-6-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-6-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-6-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-6-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-7-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-7-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-7-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-7-4-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-8-1-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-8-2-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-8-3-Tiles-v0\n",
    "    ppaquette/SuperMarioBros-8-4-Tiles-v0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env  의  reset 함수를 이용하여 슈퍼마리오의 초기화 상태를 화면으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actions and input size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 슈퍼마리오 환경의 action MultiDiscrete 6 의 형태로 이루어 져있고, input size 는 224,256,3 입니다. \n",
    "\n",
    "이는 env.action_space와  env.observation_space 함수를 통해 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiDiscrete6\n",
      "Box(224, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper  사용해서 action 형태 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 환경에서 슈퍼마리오는 [0,0,0,0,0,0] 과 같은 일차원 array로 action을 선택합니다. 예를 들면 [0,0,0,1,0,0] 의 배열을 action으로 받으면 슈퍼마리오는 오른쪽으로 움직입니다. \n",
    "현재 2차원 이상의 배열로 이루어져 있는 action을 1차원의 배열로 줄여주는 작업이 필요하고, 이는 wrapper  함수를 통하여 바꿀수 있습니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper import action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wra_act=action_space\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시한번 env.action_space 함수를 통해 확인해보면 Discre14로 바뀐것을 볼 수 있습니다 ^^\n",
    "\n",
    "Discrete14 의 의미는 마리오가 환경에서 할 수 있는 action(가만히 있는다, 위, 아래, 왼쪽, 오른쪽, 점프, 불꽃쏘기) 버튼 조합을 14가지로 나타낸 것 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(14)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper  사용해서 input size  줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습에서 발생하는 이슈중 하나는 학습시간이 오래 걸리는 것인데요. \n",
    "이를 줄여주기 위해 마찬가지로 (  ) 를 통해 인풋 사이즈를 줄여주는 작업을 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce pixel\n",
    "\n",
    "\n",
    "env=wra_act.ProcessFrame84(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observation_space 함수를 통해 다시 확인해보면, (84,84,1)로 인푸싸이즈가 줄여져 있는것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(84, 84, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "reshape_obs=np.reshape([obs],(1,84,84,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make history using the observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.stack((obs,obs,obs,obs), axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.reshape([history], (1, 84, 84, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.append(reshape_obs, history[:,:,:,:3], axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_obs=np.reshape([history],(84,84,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy 한  action 을 선택하게 해주는 함수를 만들어 줍니다. \n",
    "\n",
    "numpy의  np.random.rand()함수를 통해 epsilon 값보다 적은 값이 나올땐 0 부터 5까지의 임의의 값을 리턴해주고, \n",
    "\n",
    "epsilone 보다 큰 값이 나왔을때는 keras.predict  q value 의 값을 가장 크게 만드는 action을 리턴하게 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an greedy action\n",
    "epsilon=0.9\n",
    "action_size=6\n",
    "\n",
    "import random\n",
    "\n",
    "def get_action(history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        else:\n",
    "            q_value = model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "action=get_action(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change action(number) to the action( action what I think I need )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=(\n",
    "  [0,0,0,1,0,0], [0,0,0,1,1,0],[0,0,0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action==0:\n",
    "    actions=[0,0,0,1,0,0]\n",
    "elif action==2:\n",
    "    actions=[0,0,0,0,1,0]\n",
    "    \n",
    "elif action==1:\n",
    "    actions=[0,0,0,1,1,0]\n",
    "elif action==3:\n",
    "    actions=[0,0,0,1,1,0]\n",
    "elif action==4:\n",
    "    actions=[0,0,0,1,0,0]\n",
    "    \n",
    "elif action==5:\n",
    "    actions=[0,0,0,0,1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env.step(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make my OWN! A.I. SuperMario\n",
    "\n",
    "\n",
    "이제부터 DQN을 사용하여 장애물을 피해 Goal 까지 가는 마리오를 만들어 보도록 하겠습니다. 이곳에서부터는 위의 커널에서 리스타트를 누르신 후 다시 실행 부탁드립니다 ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import random\n",
    "\n",
    "import numpy as np \n",
    "import gym \n",
    "from wrapper import action_space\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Input, Lambda, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential\n",
    "#from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sum_tree import SumTree\n",
    "import random\n",
    "\n",
    "\n",
    "load_model = True\n",
    "\n",
    "epsilon = 1.\n",
    "epsilon_start, epsilon_end = 1.0, 0.1\n",
    "exploration_steps = 400000.\n",
    "\n",
    "epsilon_decay_step = (epsilon_start - epsilon_end) \\\n",
    "                                  / exploration_steps\n",
    "batch_size = 32\n",
    "\n",
    "discount_factor = 0.99\n",
    "\n",
    "train_start= 100\n",
    "update_target_rate = 10000\n",
    "memory=deque(maxlen=1000000)\n",
    "just_start_train=True\n",
    "state_size = (84,84,4)\n",
    "action_size =6\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(8,8),strides =(4,4), activation='relu', input_shape=state_size))\n",
    "    model.add(Conv2D(64,(4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3), strides =(1,1), activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(action_size))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "target_model=build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change action(number) to the action( action what I think I need )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "action_size=6\n",
    "epsilon = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making an greedy action\n",
    "epsilon=0.9\n",
    "action_size=6\n",
    "def get_action(history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        else:\n",
    "            q_value = model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(history, action, reward, next_history, dead):\n",
    "    memory.append((history,action,reward,next_history,dead))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(model.trainable_weights, [], loss)\n",
    "        train = K.function([model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model():\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "(Batch from the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.9\n",
    "\n",
    "def train_model():\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon -= epsilon_decay_step\n",
    "    \n",
    "    mini_batch= random.sample(memory, batch_size)\n",
    "    #mini_batch = memory.sample(batch_size)\n",
    "    \n",
    "  \n",
    "    history = np.zeros((batch_size, state_size[0],\n",
    "                            state_size[1], state_size[2]))\n",
    "    next_history = np.zeros((batch_size, state_size[0],\n",
    "                             state_size[1], state_size[2]))\n",
    "    target = np.zeros((batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "    target_value = target_model.predict(next_history)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = reward[i]\n",
    "        else:\n",
    "            target[i] = reward[i] + discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "    loss = optimizer([history, action, target])\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LET's make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "wra_act=action_space\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)\n",
    "\n",
    "#reduce inputsize to 84,84,1\n",
    "env=wra_act.ProcessFrame84(env)\n",
    "\n",
    "\n",
    "obs=env.reset()\n",
    "\n",
    "\n",
    "for e in range(10000):\n",
    "        done = False\n",
    "        \n",
    "        step, score, start_life = 0, 0, 5\n",
    "        \n",
    "        \n",
    "        \n",
    "        reshape_obs=np.reshape([obs],(1,84,84,1))\n",
    "        history=np.stack((obs,obs,obs,obs), axis = 2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        history=np.append(reshape_obs, history[:,:,:,:3], axis=3)\n",
    "        #check\n",
    "       # history=np.reshape([history],(84,84,4))\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "        while not done:\n",
    "           \n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 0: stop, 3: left, 4: left jump, 7:right, 8:right jump 11: jump\n",
    "            action=get_action(history)\n",
    "            if action==0:\n",
    "                actions=[0,0,0,1,0,0]\n",
    "            elif action==2:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "    \n",
    "            elif action==1:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==3:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==4:\n",
    "                actions=[0,0,0,1,0,0]\n",
    "    \n",
    "            elif action==5:\n",
    "                actions=[0,0,0,1,1,0]\n",
    "            elif action==6:\n",
    "                actions=[0,0,0,0,1,0]\n",
    "    \n",
    "            env.step(actions)\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, clear= env.step(actions)\n",
    "\n",
    "            \n",
    "            next_state = observe\n",
    "            \n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            \n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "\n",
    "            \n",
    "            append_sample(history, action, reward, next_history, done)\n",
    "\n",
    "    \n",
    "\n",
    "            if len(memory)>= train_start:\n",
    "                train_model()\n",
    "                \n",
    "            if global_step % update_target_rate ==0:\n",
    "                update_target_model()\n",
    "            \n",
    "            history = next_history\n",
    "            \n",
    " \n",
    "            if e % 100 == 0:\n",
    "                model.save_weights(\"./save_ws/supermario_again_ws_v6.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
