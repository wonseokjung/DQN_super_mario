{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym \n",
    "from wrapper import action_space\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Input, Lambda, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "#from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sum_tree import SumTree\n",
    "import random\n",
    "\n",
    "\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wra_act=action_space\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce pixel\n",
    "\n",
    "\n",
    "env=wra_act.ProcessFrame84(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ..., \n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#environment reset\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to Jump!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action=[0,0,0,0,1,0]\n",
    "\n",
    "for i in range(100):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  try to go forward and jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action=[0,0,0,1,0,0]\n",
    "action2=[0,0,0,1,1,0]\n",
    "action3=[0,0,0,1,1,0]\n",
    "\n",
    "for i in range(100):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### go Forward and long Jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action=[0,0,0,1,0,0]\n",
    "action2=[0,0,0,1,1,0]\n",
    "action3=[0,0,0,0,1,0]\n",
    "\n",
    "for i in range(100):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)\n",
    "    env.step(action2)\n",
    "    env.step(action2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go to the goal manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action=[0,0,0,1,0,0]\n",
    "action2=[0,0,0,1,1,0]\n",
    "action3=[0,0,0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30\n",
    "              ):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)\n",
    "    env.step(action2)\n",
    "    env.step(action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10\n",
    "              ):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)\n",
    "    env.step(action2)\n",
    "    env.step(action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25\n",
    "              ):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)\n",
    "    env.step(action2)\n",
    "    env.step(action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28\n",
    "              ):\n",
    "    env.step(action)\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)\n",
    "    env.step(action2)\n",
    "    env.step(action2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANUALLY SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    env.step(action2)\n",
    "    env.step(action3)\n",
    "    env.step(action3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making action array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=(\n",
    "    [0, 0, 0, 0, 0, 0],[1, 0, 0, 0, 0, 0],[0, 0, 1, 0, 0, 0],[0, 1, 0, 0, 0, 0],  [0, 1, 0, 0, 1, 0],  [0, 1, 0, 0, 0, 1],[0, 1, 0, 0, 1, 1], \n",
    "    [0, 0, 0, 1, 0, 0],[0, 0, 0, 1, 1, 0],  \n",
    "    [0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 1, 1],  \n",
    "     [0, 0, 0, 0, 1, 0],[0, 0, 0, 0, 0, 1], \n",
    "     [0, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilon=0.99\n",
    "def get_action():\n",
    "    \n",
    "\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        q_value = model.prediction(history)\n",
    "        return np.argmax(q_value[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Check if the get_action is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0f60c89108e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-537d55eb3ce7>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "epsilon= 0.9\n",
    "\n",
    "for i in range(100):\n",
    "    action=get_action()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "action_size=6\n",
    "epsilon = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(history, action, reward, next_history, done):\n",
    "    #TD ERROR \n",
    "    target=model.predict([history])\n",
    "    old_val = target[0][action]\n",
    "    target_val = target_model.predict(([next_history]))\n",
    "    \n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + discount_factor *  (np.amax(target_val[0]))\n",
    "    error= abs(old_val - target[0][action1])\n",
    "    memory.add(error,(history,action, reward, next_history, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples from replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon -= epsilon_decay_step\n",
    "    mini_batch = memory.sample(batch_size)\n",
    "    \n",
    "    errors= np.zeros(batch_size)\n",
    "    history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "    next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                             self.state_size[1], self.state_size[2]))\n",
    "    target = np.zeros((self.batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][1][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][1][3] / 255.)\n",
    "            action.append(mini_batch[i][1][1])\n",
    "            reward.append(mini_batch[i][1][2])\n",
    "            dead.append(mini_batch[i][1][4])\n",
    "\n",
    "    curr_q = self.model.predict(history)\n",
    "    value = self.model.predict(next_history)\n",
    "    target_value = self.target_model.predict(next_history)\n",
    "    for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        target_value[i][np.argmax(value[i])]\n",
    "            errors[i] = abs(curr_q[i][action[i]] - target[i])\n",
    "\n",
    "        # TD-error로 priority 업데이트\n",
    "    for i in range(self.batch_size):\n",
    "            idx = mini_batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "    loss = optimizer([history, action, target])\n",
    "    avg_loss += loss[0]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store (sara) at in SumTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make observe to gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's make my own A.I. SuperMario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env\n",
    "load_model=False\n",
    "\n",
    "#set the state size \n",
    "\n",
    "state_size = (84,84,4)\n",
    "#set the action size which is up, down ,left ,right , A, and B\n",
    "action_size = 6\n",
    "#Epsilone is start from 1 and end at 0.1  \n",
    "epsilon=1\n",
    "epsilon_start, epsilon_end = 1.0,0.1\n",
    "\n",
    "exploration_steps = 400000.\n",
    "#epsilon decay\n",
    "epsilon_decay_step = (epsilon_start - epsilon_end) / exploration_steps\n",
    "\n",
    "#Sampling \n",
    "batch_size = 32\n",
    "\n",
    "train_start = 50000\n",
    "update_target_rate = 10000\n",
    "discount_factor =0.99\n",
    "\n",
    "#size of the replay memory(Maximum)\n",
    "memory = deque(maxlen=400000)\n",
    "\n",
    "#build the deep learning model \n",
    "\n",
    "def build_model():\n",
    "    input=Input(shape=state_size)\n",
    " \n",
    "    shared = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "    shared = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(shared)\n",
    "    shared = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(shared)\n",
    "    flatten = Flatten()(shared)\n",
    "    \n",
    "    #network seperate state value and advantages\n",
    "    \n",
    "    advantage_fc=Dense(512, activation='relu')(flatten)\n",
    "    advantage = Dense(action_size)(advantage_fc)\n",
    "    advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True),\n",
    "                           output_shape=(action_size,))(advantage)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    value_fc = Dense(512, activation='relu')(flatten)\n",
    "    value = Dense(1)(value_fc)\n",
    "    value=Lambda(lambda s: K.expand_dims(s[:,0], -1), output_shape=(action_size,))(value)\n",
    "    \n",
    "    \n",
    "    #network merged and make Q value\n",
    "    q_value=merge([value, advantage], mode='sum')\n",
    "    model = Model(inputs=input, outputs=q_value)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "model=build_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a SUPERR MaRIOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EPISODES = 10\n",
    "\n",
    "\n",
    "# 브레이크아웃에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, env):\n",
    "        self.env = env\n",
    "        self.load_model = False\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 400000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        # 리플레이 메모리, 최대 크기 400000\n",
    "        self.memory = Memory(400000)\n",
    "        # 모델과 타겟모델을 생성하고 타겟모델 초기화\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_dtarget_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        # 텐서보드 설정\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = \\\n",
    "            self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            'summary/supermario_per', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/breakout_dqn.h5\")\n",
    "\n",
    "    # Huber Loss를 이용하기 위해 최적화 함수를 직접 정의\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        input = Input(shape=self.state_size)\n",
    "        shared = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "        shared = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(shared)\n",
    "        shared = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(shared)\n",
    "        flatten = Flatten()(shared)\n",
    "\n",
    "        # network separate state value and advantages\n",
    "        advantage_fc = Dense(512, activation='relu')(flatten)\n",
    "        advantage = Dense(self.action_size)(advantage_fc)\n",
    "        advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True),\n",
    "                           output_shape=(self.action_size,))(advantage)\n",
    "\n",
    "        value_fc = Dense(512, activation='relu')(flatten)\n",
    "        value = Dense(1)(value_fc)\n",
    "        value = Lambda(lambda s: K.expand_dims(s[:, 0], -1),\n",
    "                       output_shape=(self.action_size,))(value)\n",
    "\n",
    "        # network merged and make Q Value\n",
    "        q_value = merge([value, advantage], mode='sum')\n",
    "        model = Model(inputs=input, outputs=q_value)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # 타겟 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, history, action, reward, next_history, done):\n",
    "        # TD-error 를 구해서 같이 메모리에 저장\n",
    "        target = self.model.predict([history])\n",
    "        old_val = target[0][action]\n",
    "        target_val = self.target_model.predict([next_history])\n",
    "\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.discount_factor * (\n",
    "                np.amax(target_val[0]))\n",
    "        error = abs(old_val - target[0][action])\n",
    "\n",
    "        self.memory.add(error, (history, action, reward, next_history, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        errors = np.zeros(self.batch_size)\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][1][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][1][3] / 255.)\n",
    "            action.append(mini_batch[i][1][1])\n",
    "            reward.append(mini_batch[i][1][2])\n",
    "            dead.append(mini_batch[i][1][4])\n",
    "\n",
    "        curr_q = self.model.predict(history)\n",
    "        value = self.model.predict(next_history)\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        target_value[i][np.argmax(value[i])]\n",
    "            errors[i] = abs(curr_q[i][action[i]] - target[i])\n",
    "\n",
    "        # TD-error로 priority 업데이트\n",
    "        for i in range(self.batch_size):\n",
    "            idx = mini_batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    # 각 에피소드 당 학습 정보를 기록\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "\n",
    "# stored as ( s, a, r, s_ ) in SumTree\n",
    "class Memory:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "\n",
    "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경과 DQN 에이전트 생성\n",
    "    env = env\n",
    "    agent = DQNAgent(action_size=6, env=env)\n",
    "\n",
    "    scores, episodes, global_step = [], [], 0\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        max_x = 0\n",
    "        now_x = 0\n",
    "        hold_frame = 0\n",
    "        before_max_x = 200\n",
    "\n",
    "        start_position = 500\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        state = observe\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        action_count = 0\n",
    "        real_action, action = 0, 0\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 0: stop, 3: left, 4: left jump, 7:right, 8:right jump 11: jump\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 0\n",
    "            elif action == 1:\n",
    "                real_action = 3\n",
    "            elif action == 2:\n",
    "                real_action = 4\n",
    "            elif action == 3:\n",
    "                real_action = 7\n",
    "            elif action == 4:\n",
    "                real_action = 8\n",
    "            else:\n",
    "                real_action = 11\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, clear, max_x, timeout, now_x = \\\n",
    "                env.step(real_action)\n",
    "\n",
    "            if now_x >= 8776:\n",
    "                reward = 300\n",
    "                done = True\n",
    "\n",
    "            if done and now_x < 8776:\n",
    "                reward = -100\n",
    "\n",
    "            reward /= 100\n",
    "            # reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "            agent.append_sample(history, action, reward, next_history, done)\n",
    "\n",
    "            if global_step >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            score += reward\n",
    "            history = next_history\n",
    "\n",
    "            if now_x <= before_max_x:\n",
    "                hold_frame += 1\n",
    "                if hold_frame > 2000:\n",
    "                    break\n",
    "            else:\n",
    "                hold_frame = 0\n",
    "                before_max_x = max_x\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                            agent.summary_placeholders[i]: float(stats[i])\n",
    "                        })\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  epsilon:\", agent.epsilon,\n",
    "                      \"  global_step:\", global_step, \"  average_q:\",\n",
    "                      agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                      agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        # 1000 에피소드마다 모델 저장\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/supermario_per.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
