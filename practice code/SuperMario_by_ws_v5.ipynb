{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym \n",
    "from wrapper import action_space\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Input, Lambda, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "#from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sum_tree import SumTree\n",
    "import random\n",
    "\n",
    "\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wra_act=action_space\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce pixel\n",
    "\n",
    "\n",
    "env=wra_act.ProcessFrame84(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking shape of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_obs=np.reshape([obs],(1,84,84,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make history using the observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.stack((obs,obs,obs,obs), axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.reshape([history], (1, 84, 84, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.append(reshape_obs, history[:,:,:,:3], axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_obs=np.reshape([history],(84,84,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_act(make_obs_ph, q_func, num_actions, scope=\"deepq\", reuse=None):\n",
    "  \"\"\"Creates the act function:\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  make_obs_ph: str -> tf.placeholder or TfInput\n",
    "      a function that take a name and creates a placeholder of input with that name\n",
    "  q_func: (tf.Variable, int, str, bool) -> tf.Variable\n",
    "      the model that takes the following inputs:\n",
    "          observation_in: object\n",
    "              the output of observation placeholder\n",
    "          num_actions: int\n",
    "              number of actions\n",
    "          scope: str\n",
    "          reuse: bool\n",
    "              should be passed to outer variable scope\n",
    "      and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "  num_actions: int\n",
    "      number of actions.\n",
    "  scope: str or VariableScope\n",
    "      optional scope for variable_scope.\n",
    "  reuse: bool or None\n",
    "      whether or not the variables should be reused. To be able to reuse the scope must be given.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  act: (tf.Variable, bool, float) -> tf.Variable\n",
    "      function to select and action given observation.\n",
    "`       See the top of the file for details.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, reuse=reuse):\n",
    "    observations_ph = U.ensure_tf_input(make_obs_ph(\"observation\"))\n",
    "    stochastic_ph = tf.placeholder(tf.bool, (), name=\"stochastic\")\n",
    "    update_eps_ph = tf.placeholder(tf.float32, (), name=\"update_eps\")\n",
    "\n",
    "    eps = tf.get_variable(\"eps\", (), initializer=tf.constant_initializer(0))\n",
    "\n",
    "    q_values = q_func(observations_ph.get(), num_actions, scope=\"q_func\")\n",
    "    deterministic_actions = tf.argmax(q_values, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(observations_ph.get())[0]\n",
    "    random_actions = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=num_actions, dtype=tf.int64)\n",
    "    chose_random = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < eps\n",
    "    stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)\n",
    "\n",
    "    output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)\n",
    "    update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))\n",
    "    act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph],\n",
    "                     outputs=output_actions,\n",
    "                     givens={update_eps_ph: -1.0, stochastic_ph: True},\n",
    "                     updates=[update_eps_expr])\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       ..., \n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_act() missing 2 required positional arguments: 'q_func' and 'num_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4d1e12a745bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: build_act() missing 2 required positional arguments: 'q_func' and 'num_actions'"
     ]
    }
   ],
   "source": [
    "action=build_act(np.array(processed_obs)[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ..., \n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ..., \n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#environment reset\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to Jump!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilon=0.99\n",
    "def get_action():\n",
    "    \n",
    "\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        q_value = model.prediction(history)\n",
    "        return np.argmax(q_value[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Check if the get_action is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cannot call env.step() before calling reset()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0f60c89108e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot call env.step() before calling reset()"
     ]
    }
   ],
   "source": [
    "epsilon= 0.9\n",
    "\n",
    "for i in range(100):\n",
    "    action=get_action()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "action_size=6\n",
    "epsilon = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(history, action, reward, next_history, done):\n",
    "    #TD ERROR \n",
    "    target=model.predict([history])\n",
    "    old_val = target[0][action]\n",
    "    target_val = target_model.predict(([next_history]))\n",
    "    \n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + discount_factor *  (np.amax(target_val[0]))\n",
    "    error= abs(old_val - target[0][action1])\n",
    "    memory.add(error,(history,action, reward, next_history, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples from replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon -= epsilon_decay_step\n",
    "    mini_batch = memory.sample(batch_size)\n",
    "    \n",
    "    errors= np.zeros(batch_size)\n",
    "    history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "    next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                             self.state_size[1], self.state_size[2]))\n",
    "    target = np.zeros((self.batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][1][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][1][3] / 255.)\n",
    "            action.append(mini_batch[i][1][1])\n",
    "            reward.append(mini_batch[i][1][2])\n",
    "            dead.append(mini_batch[i][1][4])\n",
    "\n",
    "    curr_q = self.model.predict(history)\n",
    "    value = self.model.predict(next_history)\n",
    "    target_value = self.target_model.predict(next_history)\n",
    "    for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        target_value[i][np.argmax(value[i])]\n",
    "            errors[i] = abs(curr_q[i][action[i]] - target[i])\n",
    "\n",
    "        # TD-error로 priority 업데이트\n",
    "    for i in range(self.batch_size):\n",
    "            idx = mini_batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "    loss = optimizer([history, action, target])\n",
    "    avg_loss += loss[0]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store (sara) at in SumTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make observe to gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's make my own A.I. SuperMario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 20, 20, 32)   8224        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 9, 9, 64)     32832       conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 3136)         0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          1606144     flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512)          1606144     flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            513         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 6)            3078        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 6)            0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 6)            0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 6)            0           lambda_2[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,293,863\n",
      "Trainable params: 3,293,863\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:52: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/wonseok/.local/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "env\n",
    "load_model=False\n",
    "\n",
    "#set the state size \n",
    "\n",
    "state_size = (84,84,4)\n",
    "#set the action size which is up, down ,left ,right , A, and B\n",
    "action_size = 6\n",
    "#Epsilone is start from 1 and end at 0.1  \n",
    "epsilon=1\n",
    "epsilon_start, epsilon_end = 1.0,0.1\n",
    "\n",
    "exploration_steps = 400000.\n",
    "#epsilon decay\n",
    "epsilon_decay_step = (epsilon_start - epsilon_end) / exploration_steps\n",
    "\n",
    "#Sampling \n",
    "batch_size = 32\n",
    "\n",
    "train_start = 50000\n",
    "update_target_rate = 10000\n",
    "discount_factor =0.99\n",
    "\n",
    "#size of the replay memory(Maximum)\n",
    "memory = deque(maxlen=400000)\n",
    "\n",
    "#build the deep learning model \n",
    "\n",
    "def build_model():\n",
    "    input=Input(shape=state_size)\n",
    " \n",
    "    shared = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "    shared = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(shared)\n",
    "    shared = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(shared)\n",
    "    flatten = Flatten()(shared)\n",
    "    \n",
    "    #network seperate state value and advantages\n",
    "    \n",
    "    advantage_fc=Dense(512, activation='relu')(flatten)\n",
    "    advantage = Dense(action_size)(advantage_fc)\n",
    "    advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True),\n",
    "                           output_shape=(action_size,))(advantage)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    value_fc = Dense(512, activation='relu')(flatten)\n",
    "    value = Dense(1)(value_fc)\n",
    "    value=Lambda(lambda s: K.expand_dims(s[:,0], -1), output_shape=(action_size,))(value)\n",
    "    \n",
    "    \n",
    "    #network merged and make Q value\n",
    "    q_value=merge([value, advantage], mode='sum')\n",
    "    model = Model(inputs=input, outputs=q_value)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "model=build_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(84, 84, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a SUPERR MaRIOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EPISODES = 10\n",
    "\n",
    "\n",
    "# 브레이크아웃에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, env):\n",
    "        self.env = env\n",
    "        self.load_model = False\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 400000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \\\n",
    "                                  / self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        # 리플레이 메모리, 최대 크기 400000\n",
    "        self.memory = Memory(400000)\n",
    "        # 모델과 타겟모델을 생성하고 타겟모델 초기화\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_dtarget_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        # 텐서보드 설정\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = \\\n",
    "            self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            'summary/supermario_per', self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/breakout_dqn.h5\")\n",
    "\n",
    "    # Huber Loss를 이용하기 위해 최적화 함수를 직접 정의\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        input = Input(shape=self.state_size)\n",
    "        shared = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "        shared = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(shared)\n",
    "        shared = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(shared)\n",
    "        flatten = Flatten()(shared)\n",
    "\n",
    "        # network separate state value and advantages\n",
    "        advantage_fc = Dense(512, activation='relu')(flatten)\n",
    "        advantage = Dense(self.action_size)(advantage_fc)\n",
    "        advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True),\n",
    "                           output_shape=(self.action_size,))(advantage)\n",
    "\n",
    "        value_fc = Dense(512, activation='relu')(flatten)\n",
    "        value = Dense(1)(value_fc)\n",
    "        value = Lambda(lambda s: K.expand_dims(s[:, 0], -1),\n",
    "                       output_shape=(self.action_size,))(value)\n",
    "\n",
    "        # network merged and make Q Value\n",
    "        q_value = merge([value, advantage], mode='sum')\n",
    "        model = Model(inputs=input, outputs=q_value)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # 타겟 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, history, action, reward, next_history, done):\n",
    "        # TD-error 를 구해서 같이 메모리에 저장\n",
    "        target = self.model.predict([history])\n",
    "        old_val = target[0][action]\n",
    "        target_val = self.target_model.predict([next_history])\n",
    "\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.discount_factor * (\n",
    "                np.amax(target_val[0]))\n",
    "        error = abs(old_val - target[0][action])\n",
    "\n",
    "        self.memory.add(error, (history, action, reward, next_history, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "        errors = np.zeros(self.batch_size)\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][1][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][1][3] / 255.)\n",
    "            action.append(mini_batch[i][1][1])\n",
    "            reward.append(mini_batch[i][1][2])\n",
    "            dead.append(mini_batch[i][1][4])\n",
    "\n",
    "        curr_q = self.model.predict(history)\n",
    "        value = self.model.predict(next_history)\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * \\\n",
    "                                        target_value[i][np.argmax(value[i])]\n",
    "            errors[i] = abs(curr_q[i][action[i]] - target[i])\n",
    "\n",
    "        # TD-error로 priority 업데이트\n",
    "        for i in range(self.batch_size):\n",
    "            idx = mini_batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    # 각 에피소드 당 학습 정보를 기록\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "\n",
    "# stored as ( s, a, r, s_ ) in SumTree\n",
    "class Memory:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append((idx, data))\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "\n",
    "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경과 DQN 에이전트 생성\n",
    "    env = env\n",
    "    agent = DQNAgent(action_size=6, env=env)\n",
    "\n",
    "    scores, episodes, global_step = [], [], 0\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        max_x = 0\n",
    "        now_x = 0\n",
    "        hold_frame = 0\n",
    "        before_max_x = 200\n",
    "\n",
    "        start_position = 500\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        state = observe\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        action_count = 0\n",
    "        real_action, action = 0, 0\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 0: stop, 3: left, 4: left jump, 7:right, 8:right jump 11: jump\n",
    "            action = agent.get_action(history)\n",
    "            if action == 0:\n",
    "                real_action = 0\n",
    "            elif action == 1:\n",
    "                real_action = 3\n",
    "            elif action == 2:\n",
    "                real_action = 4\n",
    "            elif action == 3:\n",
    "                real_action = 7\n",
    "            elif action == 4:\n",
    "                real_action = 8\n",
    "            else:\n",
    "                real_action = 11\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, clear, max_x, timeout, now_x = \\\n",
    "                env.step(real_action)\n",
    "\n",
    "            if now_x >= 8776:\n",
    "                reward = 300\n",
    "                done = True\n",
    "\n",
    "            if done and now_x < 8776:\n",
    "                reward = -100\n",
    "\n",
    "            reward /= 100\n",
    "            # reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(\n",
    "                agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "            agent.append_sample(history, action, reward, next_history, done)\n",
    "\n",
    "            if global_step >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "            if global_step % agent.update_target_rate == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            score += reward\n",
    "            history = next_history\n",
    "\n",
    "            if now_x <= before_max_x:\n",
    "                hold_frame += 1\n",
    "                if hold_frame > 2000:\n",
    "                    break\n",
    "            else:\n",
    "                hold_frame = 0\n",
    "                before_max_x = max_x\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                if global_step > agent.train_start:\n",
    "                    stats = [score, agent.avg_q_max / float(step), step,\n",
    "                             agent.avg_loss / float(step)]\n",
    "                    for i in range(len(stats)):\n",
    "                        agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                            agent.summary_placeholders[i]: float(stats[i])\n",
    "                        })\n",
    "                    summary_str = agent.sess.run(agent.summary_op)\n",
    "                    agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  epsilon:\", agent.epsilon,\n",
    "                      \"  global_step:\", global_step, \"  average_q:\",\n",
    "                      agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                      agent.avg_loss / float(step))\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        # 1000 에피소드마다 모델 저장\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/supermario_per.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
